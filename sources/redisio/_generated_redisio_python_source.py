# ==============================================================================
# Merged Lakeflow Source: redis
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Tuple,
)
import json
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import redis


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/redis/redis.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict) -> None:
            """
            Initialize the Redis connector with connection parameters.

            Args:
                options: Dictionary containing:
                    - host: Redis server hostname (default: localhost)
                    - port: Redis server port (default: 6379)
                    - password: Optional password for authentication
                    - db: Database number 0-15 (default: 0)
                    - ssl: Whether to use SSL (default: false)
                    - key_pattern: Optional glob pattern to filter keys (default: *)
                    - batch_size: Number of keys to scan per batch (default: 1000)
            """
            self.host = options.get("host", "localhost")
            self.port = int(options.get("port", 6379))
            self.password = options.get("password", None)
            self.db = int(options.get("db", 0))
            self.ssl = options.get("ssl", "false").lower() == "true"
            self.key_pattern = options.get("key_pattern", "*")
            self.batch_size = int(options.get("batch_size", 1000))

            # Initialize Redis client
            self._client = redis.Redis(
                host=self.host,
                port=self.port,
                password=self.password,
                db=self.db,
                ssl=self.ssl,
                decode_responses=True,
            )

            # Cache for schemas
            self._schema_cache = {}

            # Centralized object metadata configuration
            self._object_config = {
                "keys": {
                    "primary_keys": ["key_name"],
                    "cursor_field": "scanned_at",
                    "ingestion_type": "snapshot",
                },
                "strings": {
                    "primary_keys": ["key_name"],
                    "cursor_field": "scanned_at",
                    "ingestion_type": "snapshot",
                },
                "hashes": {
                    "primary_keys": ["key_name", "field"],
                    "cursor_field": "scanned_at",
                    "ingestion_type": "snapshot",
                },
                "lists": {
                    "primary_keys": ["key_name", "list_index"],
                    "cursor_field": "scanned_at",
                    "ingestion_type": "snapshot",
                },
                "sets": {
                    "primary_keys": ["key_name", "member"],
                    "cursor_field": "scanned_at",
                    "ingestion_type": "snapshot",
                },
                "sorted_sets": {
                    "primary_keys": ["key_name", "member"],
                    "cursor_field": "scanned_at",
                    "ingestion_type": "snapshot",
                },
                "streams": {
                    "primary_keys": ["key_name", "entry_id"],
                    "cursor_field": "entry_id",
                    "ingestion_type": "append",
                },
                "server_info": {
                    "primary_keys": ["section", "property"],
                    "cursor_field": "collected_at",
                    "ingestion_type": "snapshot",
                },
            }

            # Centralized schema configuration
            self._schema_config = {
                "keys": StructType(
                    [
                        StructField("key_name", StringType(), False),
                        StructField("key_type", StringType(), True),
                        StructField("ttl_seconds", LongType(), True),
                        StructField("memory_bytes", LongType(), True),
                        StructField("encoding", StringType(), True),
                        StructField("refcount", LongType(), True),
                        StructField("idle_time_seconds", LongType(), True),
                        StructField("scanned_at", LongType(), True),
                    ]
                ),
                "strings": StructType(
                    [
                        StructField("key_name", StringType(), False),
                        StructField("value", StringType(), True),
                        StructField("value_length", LongType(), True),
                        StructField("ttl_seconds", LongType(), True),
                        StructField("scanned_at", LongType(), True),
                    ]
                ),
                "hashes": StructType(
                    [
                        StructField("key_name", StringType(), False),
                        StructField("field", StringType(), False),
                        StructField("value", StringType(), True),
                        StructField("ttl_seconds", LongType(), True),
                        StructField("scanned_at", LongType(), True),
                    ]
                ),
                "lists": StructType(
                    [
                        StructField("key_name", StringType(), False),
                        StructField("list_index", LongType(), False),
                        StructField("value", StringType(), True),
                        StructField("list_length", LongType(), True),
                        StructField("ttl_seconds", LongType(), True),
                        StructField("scanned_at", LongType(), True),
                    ]
                ),
                "sets": StructType(
                    [
                        StructField("key_name", StringType(), False),
                        StructField("member", StringType(), False),
                        StructField("set_cardinality", LongType(), True),
                        StructField("ttl_seconds", LongType(), True),
                        StructField("scanned_at", LongType(), True),
                    ]
                ),
                "sorted_sets": StructType(
                    [
                        StructField("key_name", StringType(), False),
                        StructField("member", StringType(), False),
                        StructField("score", DoubleType(), True),
                        StructField("rank", LongType(), True),
                        StructField("zset_cardinality", LongType(), True),
                        StructField("ttl_seconds", LongType(), True),
                        StructField("scanned_at", LongType(), True),
                    ]
                ),
                "streams": StructType(
                    [
                        StructField("key_name", StringType(), False),
                        StructField("entry_id", StringType(), False),
                        StructField("fields", StringType(), True),
                        StructField("timestamp_ms", LongType(), True),
                        StructField("sequence", LongType(), True),
                        StructField("ttl_seconds", LongType(), True),
                    ]
                ),
                "server_info": StructType(
                    [
                        StructField("section", StringType(), False),
                        StructField("property", StringType(), False),
                        StructField("value", StringType(), True),
                        StructField("collected_at", LongType(), True),
                    ]
                ),
            }

        def list_tables(self) -> list[str]:
            """
            List available Redis tables/data types.

            Returns:
                List of supported table names
            """
            return [
                "keys",
                "strings",
                "hashes",
                "lists",
                "sets",
                "sorted_sets",
                "streams",
                "server_info",
            ]

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Get the Spark schema for a Redis table.

            Args:
                table_name: Name of the table
                table_options: Additional options (key_pattern can override default)

            Returns:
                StructType representing the table schema
            """
            if table_name not in self._schema_config:
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables are: {self.list_tables()}"
                )
            return self._schema_config[table_name]

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> dict:
            """
            Get metadata for a Redis table.

            Args:
                table_name: Name of the table
                table_options: Additional options

            Returns:
                Dictionary with primary_keys, cursor_field, and ingestion_type
            """
            if table_name not in self._object_config:
                raise ValueError(
                    f"Unsupported table: {table_name}. Supported tables are: {self.list_tables()}"
                )
            config = self._object_config[table_name]
            return {
                "primary_keys": config["primary_keys"],
                "cursor_field": config["cursor_field"],
                "ingestion_type": config["ingestion_type"],
            }

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> Tuple[List[Dict], Dict]:
            """
            Read data from a Redis table.

            Args:
                table_name: Name of the table to read
                start_offset: Dictionary containing cursor information for incremental reads
                table_options: Additional options (key_pattern can filter keys)

            Returns:
                Tuple of (records, new_offset)
            """
            if table_name not in self._object_config:
                raise ValueError(f"Unsupported table: {table_name}")

            # Get key pattern from table_options or use default
            key_pattern = table_options.get("key_pattern", self.key_pattern)

            # Route to appropriate reader
            reader_map = {
                "keys": self._read_keys,
                "strings": self._read_strings,
                "hashes": self._read_hashes,
                "lists": self._read_lists,
                "sets": self._read_sets,
                "sorted_sets": self._read_sorted_sets,
                "streams": self._read_streams,
                "server_info": self._read_server_info,
            }

            reader = reader_map.get(table_name)
            if reader:
                return reader(key_pattern, start_offset)
            else:
                raise ValueError(f"No reader implemented for table: {table_name}")

        def _scan_keys(self, pattern: str, key_type: str = None) -> Iterator[str]:
            """
            Scan Redis keys matching pattern and optionally filter by type.

            Args:
                pattern: Glob pattern for key matching
                key_type: Optional type filter (string, hash, list, set, zset, stream)

            Yields:
                Key names matching the pattern and type
            """
            cursor = 0
            while True:
                cursor, keys = self._client.scan(
                    cursor=cursor, match=pattern, count=self.batch_size
                )
                for key in keys:
                    if key_type is None:
                        yield key
                    else:
                        actual_type = self._client.type(key)
                        if actual_type == key_type:
                            yield key
                if cursor == 0:
                    break

        def _get_ttl(self, key: str) -> int:
            """Get TTL for a key, returns -1 if no expiry, -2 if key doesn't exist."""
            return self._client.ttl(key)

        def _read_keys(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read metadata about all keys matching the pattern.

            Returns key name, type, TTL, memory usage, and other metadata.
            """
            records = []
            current_time = int(time.time())

            for key in self._scan_keys(key_pattern):
                try:
                    key_type = self._client.type(key)
                    ttl = self._get_ttl(key)

                    # Get debug object info (may not be available on all Redis versions)
                    try:
                        debug_info = self._client.debug_object(key)
                        encoding = debug_info.get("encoding", None)
                        refcount = debug_info.get("refcount", None)
                        idle_time = debug_info.get("idletime", None)
                    except Exception:
                        encoding = None
                        refcount = None
                        idle_time = None

                    # Get memory usage (Redis 4.0+)
                    try:
                        memory = self._client.memory_usage(key)
                    except Exception:
                        memory = None

                    record = {
                        "key_name": key,
                        "key_type": key_type,
                        "ttl_seconds": ttl if ttl >= 0 else None,
                        "memory_bytes": memory,
                        "encoding": encoding,
                        "refcount": refcount,
                        "idle_time_seconds": idle_time,
                        "scanned_at": current_time,
                    }
                    records.append(record)
                except Exception as e:
                    # Skip keys that are deleted during scan
                    continue

            return records, {"scanned_at": current_time}

        def _read_strings(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read all string key-value pairs matching the pattern.
            """
            records = []
            current_time = int(time.time())

            for key in self._scan_keys(key_pattern, key_type="string"):
                try:
                    value = self._client.get(key)
                    ttl = self._get_ttl(key)

                    record = {
                        "key_name": key,
                        "value": value,
                        "value_length": len(value) if value else 0,
                        "ttl_seconds": ttl if ttl >= 0 else None,
                        "scanned_at": current_time,
                    }
                    records.append(record)
                except Exception:
                    continue

            return records, {"scanned_at": current_time}

        def _read_hashes(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read all hash fields from keys matching the pattern.

            Each hash field becomes a separate record.
            """
            records = []
            current_time = int(time.time())

            for key in self._scan_keys(key_pattern, key_type="hash"):
                try:
                    hash_data = self._client.hgetall(key)
                    ttl = self._get_ttl(key)

                    for field, value in hash_data.items():
                        record = {
                            "key_name": key,
                            "field": field,
                            "value": value,
                            "ttl_seconds": ttl if ttl >= 0 else None,
                            "scanned_at": current_time,
                        }
                        records.append(record)
                except Exception:
                    continue

            return records, {"scanned_at": current_time}

        def _read_lists(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read all list elements from keys matching the pattern.

            Each list element becomes a separate record with its index.
            """
            records = []
            current_time = int(time.time())

            for key in self._scan_keys(key_pattern, key_type="list"):
                try:
                    list_length = self._client.llen(key)
                    list_data = self._client.lrange(key, 0, -1)
                    ttl = self._get_ttl(key)

                    for idx, value in enumerate(list_data):
                        record = {
                            "key_name": key,
                            "list_index": idx,
                            "value": value,
                            "list_length": list_length,
                            "ttl_seconds": ttl if ttl >= 0 else None,
                            "scanned_at": current_time,
                        }
                        records.append(record)
                except Exception:
                    continue

            return records, {"scanned_at": current_time}

        def _read_sets(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read all set members from keys matching the pattern.

            Each set member becomes a separate record.
            """
            records = []
            current_time = int(time.time())

            for key in self._scan_keys(key_pattern, key_type="set"):
                try:
                    members = self._client.smembers(key)
                    cardinality = len(members)
                    ttl = self._get_ttl(key)

                    for member in members:
                        record = {
                            "key_name": key,
                            "member": member,
                            "set_cardinality": cardinality,
                            "ttl_seconds": ttl if ttl >= 0 else None,
                            "scanned_at": current_time,
                        }
                        records.append(record)
                except Exception:
                    continue

            return records, {"scanned_at": current_time}

        def _read_sorted_sets(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read all sorted set members from keys matching the pattern.

            Each member becomes a separate record with its score and rank.
            """
            records = []
            current_time = int(time.time())

            for key in self._scan_keys(key_pattern, key_type="zset"):
                try:
                    # Get all members with scores
                    members_with_scores = self._client.zrange(
                        key, 0, -1, withscores=True
                    )
                    cardinality = len(members_with_scores)
                    ttl = self._get_ttl(key)

                    for rank, (member, score) in enumerate(members_with_scores):
                        record = {
                            "key_name": key,
                            "member": member,
                            "score": score,
                            "rank": rank,
                            "zset_cardinality": cardinality,
                            "ttl_seconds": ttl if ttl >= 0 else None,
                            "scanned_at": current_time,
                        }
                        records.append(record)
                except Exception:
                    continue

            return records, {"scanned_at": current_time}

        def _read_streams(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read stream entries from keys matching the pattern.

            Supports incremental reading using stream entry IDs.
            For full refresh, reads all entries from the beginning.
            For incremental, reads entries after the last processed ID.
            """
            records = []
            latest_entry_id = start_offset.get("entry_id", "0-0") if start_offset else "0-0"
            new_latest_id = latest_entry_id

            for key in self._scan_keys(key_pattern, key_type="stream"):
                try:
                    ttl = self._get_ttl(key)

                    # Read entries after the start offset
                    # Use XREAD with > syntax for incremental
                    entries = self._client.xrange(key, min=f"({latest_entry_id}", max="+")

                    for entry_id, fields in entries:
                        # Parse timestamp and sequence from entry ID
                        parts = entry_id.split("-")
                        timestamp_ms = int(parts[0]) if len(parts) > 0 else 0
                        sequence = int(parts[1]) if len(parts) > 1 else 0

                        record = {
                            "key_name": key,
                            "entry_id": entry_id,
                            "fields": json.dumps(fields),
                            "timestamp_ms": timestamp_ms,
                            "sequence": sequence,
                            "ttl_seconds": ttl if ttl >= 0 else None,
                        }
                        records.append(record)

                        # Track the latest entry ID for offset
                        if entry_id > new_latest_id:
                            new_latest_id = entry_id
                except Exception:
                    continue

            return records, {"entry_id": new_latest_id}

        def _read_server_info(
            self, key_pattern: str, start_offset: dict
        ) -> Tuple[List[Dict], Dict]:
            """
            Read Redis server information and statistics.

            Returns server info as key-value pairs organized by section.
            """
            records = []
            current_time = int(time.time())

            try:
                info = self._client.info()

                for key, value in info.items():
                    # Determine section based on key patterns
                    section = self._categorize_info_key(key)

                    record = {
                        "section": section,
                        "property": key,
                        "value": str(value),
                        "collected_at": current_time,
                    }
                    records.append(record)
            except Exception as e:
                raise Exception(f"Failed to read server info: {str(e)}")

            return records, {"collected_at": current_time}

        def _categorize_info_key(self, key: str) -> str:
            """Categorize an INFO key into its section."""
            section_prefixes = {
                "redis_": "server",
                "os": "server",
                "arch": "server",
                "process": "server",
                "tcp_": "server",
                "uptime": "server",
                "hz": "server",
                "lru": "server",
                "executable": "server",
                "config_file": "server",
                "connected_clients": "clients",
                "client_": "clients",
                "blocked_clients": "clients",
                "tracking_clients": "clients",
                "used_memory": "memory",
                "mem_": "memory",
                "total_system_memory": "memory",
                "maxmemory": "memory",
                "allocator": "memory",
                "lazyfree": "memory",
                "active_defrag": "memory",
                "loading": "persistence",
                "rdb_": "persistence",
                "aof_": "persistence",
                "current_": "persistence",
                "total_connections": "stats",
                "total_commands": "stats",
                "instantaneous": "stats",
                "rejected_connections": "stats",
                "sync_": "stats",
                "expired_": "stats",
                "evicted_": "stats",
                "keyspace_": "stats",
                "pubsub_": "stats",
                "latest_fork_usec": "stats",
                "migrate_cached_sockets": "stats",
                "slave_": "replication",
                "master_": "replication",
                "connected_slaves": "replication",
                "repl_": "replication",
                "second_repl_offset": "replication",
                "used_cpu": "cpu",
                "cluster_": "cluster",
                "db": "keyspace",
            }

            for prefix, section in section_prefixes.items():
                if key.startswith(prefix):
                    return section

            return "other"

        def test_connection(self) -> dict:
            """
            Test the connection to Redis server.

            Returns:
                Dictionary with status and message
            """
            try:
                # Try to ping the server
                response = self._client.ping()

                if response:
                    # Get basic server info
                    info = self._client.info("server")
                    redis_version = info.get("redis_version", "unknown")

                    return {
                        "status": "success",
                        "message": f"Connected to Redis server v{redis_version}",
                    }
                else:
                    return {
                        "status": "error",
                        "message": "Server did not respond to PING",
                    }
            except redis.ConnectionError as e:
                return {
                    "status": "error",
                    "message": f"Connection failed: {str(e)}",
                }
            except redis.AuthenticationError as e:
                return {
                    "status": "error",
                    "message": f"Authentication failed: {str(e)}",
                }
            except Exception as e:
                return {"status": "error", "message": f"Connection failed: {str(e)}"}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
